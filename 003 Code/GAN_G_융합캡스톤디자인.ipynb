{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ca3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed \n",
    "from tqdm import tqdm  \n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pip install tqdm_joblib\n",
    "\n",
    "folder_path = \"/Users/naekyung/Desktop/A/00321804(0002)\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "required_features = ['name1', 'name2'] + [f'name{i:02}' for i in range(1, 15)]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for idx, file in enumerate(csv_files, start=1):\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column not in required_features:\n",
    "            df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    dataframes = {}\n",
    "    for i in range(1, 19):\n",
    "        for j in range(1, 18):\n",
    "            list_name = f'list_{i}_{j}'\n",
    "            temp_list = df[(df['rack_no'] == i) & (df['module_no'] == j)].drop(columns=['rack_no', 'module_no']).values.tolist()\n",
    "            if temp_list:\n",
    "                columns = [f'{i}_{j}_{k+1}' for k in range(len(temp_list[0]))]\n",
    "                temp_df = pd.DataFrame(temp_list, columns=columns)\n",
    "                dataframes[list_name] = temp_df\n",
    "\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes.values(), axis=1)\n",
    "        result_name = f'A_4_{idx}'\n",
    "        results[result_name] = combined_df\n",
    "\n",
    "for name, dataframe in results.items():\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(dataframe)\n",
    "\n",
    "# 모든 데이터프레임을 옆으로 병합\n",
    "combined_results_df = pd.concat(results.values(), axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Combined DataFrame:\")\n",
    "print(combined_results_df)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib  # 모델 저장을 위해 사용\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = RobustScaler()\n",
    "scaled_combined_data = scaler.fit_transform(combined_results_df)\n",
    "scaled_combined_df = pd.DataFrame(scaled_combined_data, columns=combined_results_df.columns)\n",
    "\n",
    "# 데이터 전치 (231336*1440)\n",
    "transposed_df = scaled_combined_df.T  \n",
    "\n",
    "print(\"Scaled_combined_df.T:\")\n",
    "print(transposed_df)\n",
    "\n",
    "# IPCA *****************************************************************************************************\n",
    "n_components = 400 # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<< 주성분\n",
    "batch_size = 1500\n",
    "\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "transformed_data = ipca.fit_transform(transposed_df)\n",
    "\n",
    "# 변환된 데이터를 데이터프레임으로 저장\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "print(\"Transformed DataFrame:\")\n",
    "print(transformed_df)\n",
    "\n",
    "# IPCA 모델 저장 (다른 데이터 복원에 사용하기 위해)\n",
    "joblib.dump(ipca, 'ipca_model.pkl')\n",
    "print(\"IPCA 모델이 저장되었습니다: ipca_model.pkl\")\n",
    "\n",
    "# 복원 ********************************************************************************************************\n",
    "# 역변환을 통해 데이터 복원 (역스케일링 안 함)\n",
    "reconstructed_data = ipca.inverse_transform(transformed_data)\n",
    "\n",
    "# 원래 형식으로 복원 후 다시 전치\n",
    "reconstructed_df = pd.DataFrame(reconstructed_data.T, columns=combined_results_df.columns)\n",
    "print(\"Reconstructed DataFrame:\")\n",
    "print(reconstructed_df)\n",
    "\n",
    "# MSE 계산 ****************************************************************************************************8\n",
    "\n",
    "# 원본 데이터와 복원된 데이터 간 MSE 계산\n",
    "mse = np.mean((scaled_combined_df.values - reconstructed_df.values) ** 2)\n",
    "print(\"IPCA 복원 후 평균 제곱 오차(MSE):\", mse)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 경로 설정\n",
    "file_path = \"/Users/naekyung/Desktop/blind_test/A_2.csv\"\n",
    "\n",
    "# CSV 파일 읽기\n",
    "blind_test_df = pd.read_csv(file_path)\n",
    "\n",
    "# blind 데이터 전처리 하는 부분 (기밀 생략)\n",
    "\n",
    "# 두 데이터프레임 로드\n",
    "combined_df_1 = pd.read_csv(\"combined_bsc_fg_no_1.csv\")  # bsc_fg_no=1 결과\n",
    "combined_df_2 = pd.read_csv(\"combined_bsc_fg_no_2.csv\")  # bsc_fg_no=2 결과\n",
    "\n",
    "# 가로 병합\n",
    "final_combined_df = pd.concat([combined_df_1, combined_df_2], axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"가로 병합된 최종 데이터프레임:\")\n",
    "print(final_combined_df)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "# RobustScaler를 사용하여 스케일링\n",
    "scaler = RobustScaler()\n",
    "scaled_blind_data = scaler.fit_transform(final_combined_df)  # final_combined_df 인풋으로 받음\n",
    "scaled_blind_df = pd.DataFrame(scaled_blind_data, columns=final_combined_df.columns)\n",
    "\n",
    "# 데이터 전치\n",
    "transposed_blind_df = pd.DataFrame(scaled_blind_df.T)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Transposed Blind Scaled DataFrame:\")\n",
    "print(transposed_blind_df)\n",
    "\n",
    "# 정상 데이터로 학습된 IPCA 모델 로드\n",
    "ipca = joblib.load('ipca_model.pkl')\n",
    "print(\"IPCA 모델 로드 완료: ipca_model.pkl\")\n",
    "\n",
    "# 피처 개수 검증 및 조정\n",
    "expected_features = ipca.components_.shape[1]  # IPCA 모델의 피처 개수\n",
    "actual_features = transposed_blind_df.shape[1]\n",
    "\n",
    "if actual_features < expected_features:\n",
    "    # 부족한 피처를 채움\n",
    "    for i in range(expected_features - actual_features):\n",
    "        transposed_blind_df[f'missing_feature_{i}'] = 0\n",
    "elif actual_features > expected_features:\n",
    "    # 초과하는 피처를 제거\n",
    "    transposed_blind_df = transposed_blind_df.iloc[:, :expected_features]\n",
    "\n",
    "# 데이터 복원 (각 행에 대해 복원 수행)\n",
    "reconstructed_data_list = []\n",
    "\n",
    "for index in range(transposed_blind_df.shape[0]):\n",
    "    row = transposed_blind_df.iloc[index:index+1, :].values\n",
    "    transformed_row = ipca.transform(row)  # PCA 변환\n",
    "    reconstructed_row = ipca.inverse_transform(transformed_row)  # PCA 복원\n",
    "    reconstructed_data_list.append(reconstructed_row)\n",
    "\n",
    "# 복원된 데이터를 데이터프레임으로 변환\n",
    "reconstructed_blind_df = pd.DataFrame(\n",
    "    np.vstack(reconstructed_data_list), \n",
    "    columns=transposed_blind_df.columns\n",
    ")\n",
    "\n",
    "# MSE 계산\n",
    "mse = np.mean((transposed_blind_df.values - reconstructed_blind_df.values) ** 2)\n",
    "print(\"블라인드 데이터 복원 후 평균 제곱 오차(MSE), 주성분 : 100 :\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
